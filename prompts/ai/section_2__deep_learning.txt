Please help me generate an exam consisting of multiple choice questions.

Make it very challenging since we are selecting students for the International Olympiad in Artificial Intelligence competition!!
Make sure that the correct answer is not immediately obvious, e.g. it should not always be the most lengthy choice.
Be VERY creative~

The topic is: Deep Learning

Here is the syllabus:

- Multi-Layer Perceptrons (MLP)
- Stochastic Gradient Descent (SGD), Mini-Batch Gradient Descent
- Momentum Methods (Adam, AdamW)
- Adaptive Learning Rates
- Convergence and Learning Rates
- Weight Regularization
- Early Stopping
- Dropout, Gaussian Noise
- Weight Initialization
- Batch Normalization
- Autoencoders and Sparse Encoders

Here are some example questions:

```
1. Which of the followings are true about Batch Normalization?

A) Batch Norm layers are skipped at test time because a single test example cannot be normalized.
B) Its learnable parameters can only be learned using gradient descent or mini-batch gradient descent, but not other optimization algorithms.
C) It introduces noise to a hidden layerâ€™s activation, because the mean and the standard deviation are estimated with a mini-batch of data. (CORRECT)
D) All of the Above


2. What is the benefit of using Momentum optimization?

A) Simple update rule with minimal hyperparameters
B) Helps get weights out of local minima (CORRECT)
C) Effectively scales the learning rate to act the same amount across all dimensions
D) Combines the benefits of multiple optimization methods


3. Which of the below can you implement to solve the exploding gradient problem?

A) Use SGD optimization
B) Oversample minority classes
C) Increase the batch size
D) Impose gradient clipping (CORRECT)
```
